{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "apR09HPwd4SV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, \\\n",
        "    GPT2Tokenizer"
      ],
      "metadata": {
        "id": "D_Gx6V8Rei8N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(train_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_path,\n",
        "        block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    return train_dataset, data_collator"
      ],
      "metadata": {
        "id": "5RvKi-cRej6G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, data_collator):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-viking\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "2ZO0wclrej9H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ZwA2lMej_1",
        "outputId": "4675aedb-bc0e-49db-92e3-a2a83843b525"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/ragnar_dialogues.txt'\n",
        "train_dataset, data_collator = load_dataset(train_path, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAFjmHxwekCv",
        "outputId": "a9346683-b545-496a-a759-abdead56601e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate -U"
      ],
      "metadata": {
        "id": "VQLVJZWQe8y6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_dataset, data_collator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "KU0C5xMKekFe",
        "outputId": "c4c1331d-15db-4062-c9de-904edfe836be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 02:53, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-viking\")\n",
        "tokenizer.save_pretrained(\"./gpt2-viking\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_PyTiH2ekII",
        "outputId": "b326c1fc-4c2f-4fbe-9ea7-40da7e991a3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-viking/tokenizer_config.json',\n",
              " './gpt2-viking/special_tokens_map.json',\n",
              " './gpt2-viking/vocab.json',\n",
              " './gpt2-viking/merges.txt',\n",
              " './gpt2-viking/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r gpt2-viking.zip gpt2-viking/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3BBkuq3g5UY",
        "outputId": "08192e13-e126-45d0-ad5f-744c59467822"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: gpt2-viking/ (stored 0%)\n",
            "  adding: gpt2-viking/merges.txt (deflated 53%)\n",
            "  adding: gpt2-viking/special_tokens_map.json (deflated 74%)\n",
            "  adding: gpt2-viking/config.json (deflated 51%)\n",
            "  adding: gpt2-viking/vocab.json (deflated 68%)\n",
            "  adding: gpt2-viking/generation_config.json (deflated 24%)\n",
            "  adding: gpt2-viking/runs/ (stored 0%)\n",
            "  adding: gpt2-viking/runs/Feb29_20-10-13_9a0f451559d5/ (stored 0%)\n",
            "  adding: gpt2-viking/runs/Feb29_20-10-13_9a0f451559d5/events.out.tfevents.1709237414.9a0f451559d5.1323.0 (deflated 59%)\n",
            "  adding: gpt2-viking/tokenizer_config.json (deflated 54%)\n",
            "  adding: gpt2-viking/model.safetensors (deflated 7%)\n"
          ]
        }
      ]
    }
  ]
}